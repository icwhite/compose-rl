run_name: local-llama8b-dpo
seed: 17
model:
  beta: 0.01
  name: hf_dpo_lm
  loss_type: dpo
  pretrained: true
  use_auth_token: true
  use_flash_attention_2: true
  pretrained_model_name_or_path: izzcw/llama_3.1_large_crafting_sft_success

loggers:
  mlflow:
    experiment_name: mindcraft-crafting-test
  wandb:
    project: mindcraft-dpo
    name: local-llama8b-dpo

callbacks:
  dpo: {}
  lr_monitor: {}
  speed_monitor:
    window_size: 10
  memory_monitor: {}
  # hf_checkpointer:
  #   save_folder: # TODO: insert save path for huggingface checkpoints
  #   save_interval: 1ep

optimizer:
  lr: 1.0e-07
  eps: 1.0e-10
  name: decoupled_adamw
  betas:
  - 0.9
  - 0.95
  weight_decay: 0
precision: amp_bf16

scheduler:
  name: cosine_with_warmup
  alpha_f: 0.05
  t_warmup: 0.1dur

tokenizer:
  name: meta-llama/Llama-3.1-8B-Instruct
  kwargs:
    model_max_length: ${max_seq_len}
    trust_remote_code: true
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1
autoresume: true
eval_first: false

fsdp_config:
  verbose: false
  mixed_precision: PURE
  state_dict_type: sharded
  use_orig_params: true
  limit_all_gathers: true
  sharding_strategy: FULL_SHARD
  activation_cpu_offload: false

max_seq_len: 1000
save_folder: /models/dpo_model    # TODO: update for a proper save path
dist_timeout: 600
max_duration: 10ep
progress_bar: false

train_loader:
  name: pairwise_preference
  dataset:
    # local: # TODO: insert local dataset path
    # remote: # TODO: insert remote dataset path if applicable
    split: train

    shuffle: true
    max_seq_len: ${max_seq_len}
    shuffle_seed: ${seed}
  drop_last: true
  num_workers: 8

eval_interval: 1
save_interval: 1ep
log_to_console: true
load_weights_only: true
console_log_interval: 1ba
device_eval_batch_size: 1
eval_subset_num_batches: -1
global_train_batch_size: 8    # TODO: update for more realistic batch sizes
device_train_microbatch_size: 1